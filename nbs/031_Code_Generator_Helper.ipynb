{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a26be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp _code_generator.helper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bb7431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "from typing import *\n",
    "import random\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "import functools\n",
    "import logging\n",
    "\n",
    "import openai\n",
    "from fastcore.foundation import patch\n",
    "\n",
    "from fastkafka._components.logger import get_logger, set_level\n",
    "from fastkafka._code_generator.prompts import SYSTEM_PROMPT, DEFAULT_FASTKAFKA_PROMPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a580a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytest\n",
    "import unittest.mock\n",
    "\n",
    "from fastkafka._components.logger import suppress_timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25822c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c529a82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: ok\n"
     ]
    }
   ],
   "source": [
    "suppress_timestamps()\n",
    "logger = get_logger(__name__, level=20)\n",
    "logger.info(\"ok\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b604e0ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "def set_logger_level(func):\n",
    "    @functools.wraps(func)\n",
    "    def wrapper_decorator(*args, **kwargs):\n",
    "        if (\"debug\" in kwargs) and kwargs[\"debug\"]:\n",
    "            set_level(logging.DEBUG)\n",
    "        else:\n",
    "            set_level(logging.WARNING)\n",
    "        return func(*args, **kwargs)\n",
    "    return wrapper_decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80d3b2cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger():\n",
    "    logger.debug(\"INFO\")\n",
    "    logger.info(\"WARNING\")\n",
    "\n",
    "    \n",
    "_test_logger()\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.WARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d7afe5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: INFO\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "@set_logger_level\n",
    "def _test_logger(**kwargs):\n",
    "    logger.debug(\"DEBUG\")\n",
    "    logger.info(\"INFO\")\n",
    "\n",
    "    \n",
    "_test_logger(debug=True)\n",
    "display(logger.getEffectiveLevel())\n",
    "assert logger.getEffectiveLevel() == logging.DEBUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e1d8a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "DEFAULT_PARAMS = {\n",
    "    \"temperature\": 0.7,\n",
    "}\n",
    "\n",
    "DEFAULT_MODEL = \"gpt-3.5-turbo-16k\" # gpt-3.5-turbo\n",
    "\n",
    "MAX_RETRIES = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8ee713",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "# Reference: https://github.com/openai/openai-cookbook/blob/main/examples/How_to_handle_rate_limits.ipynb\n",
    "\n",
    "\n",
    "def _retry_with_exponential_backoff(\n",
    "    initial_delay: float = 1,\n",
    "    exponential_base: float = 2,\n",
    "    jitter: bool = True,\n",
    "    max_retries: int = 10,\n",
    "    max_wait: float = 60,\n",
    "    errors: tuple = (\n",
    "        openai.error.RateLimitError,\n",
    "        openai.error.ServiceUnavailableError,\n",
    "        openai.error.APIError,\n",
    "    ),\n",
    ") -> Callable:\n",
    "    \"\"\"Retry a function with exponential backoff.\"\"\"\n",
    "\n",
    "    def decorator(\n",
    "        func: Callable[[str], Tuple[str, str]]\n",
    "    ) -> Callable[[str], Tuple[str, str]]:\n",
    "        def wrapper(*args, **kwargs):  # type: ignore\n",
    "            num_retries = 0\n",
    "            delay = initial_delay\n",
    "\n",
    "            while True:\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "\n",
    "                except errors as e:\n",
    "                    num_retries += 1\n",
    "                    if num_retries > max_retries:\n",
    "                        raise Exception(\n",
    "                            f\"Maximum number of retries ({max_retries}) exceeded.\"\n",
    "                        )\n",
    "                    delay = min(\n",
    "                        delay\n",
    "                        * exponential_base\n",
    "                        * (1 + jitter * random.random()),  # nosec\n",
    "                        max_wait,\n",
    "                    )\n",
    "                    logger.info(\n",
    "                        f\"Note: OpenAI's API rate limit reached. Command will automatically retry in {int(delay)} seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\",\n",
    "                    )\n",
    "                    time.sleep(delay)\n",
    "\n",
    "                except Exception as e:\n",
    "                    raise e\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234e0a84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "@_retry_with_exponential_backoff()\n",
    "def mock_func():\n",
    "    return \"Success\"\n",
    "\n",
    "actual = mock_func()\n",
    "expected = \"Success\"\n",
    "\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f323384",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] __main__: Note: OpenAI's API rate limit reached. Command will automatically retry in 2 seconds. For more information visit: https://help.openai.com/en/articles/5955598-is-api-usage-subject-to-any-rate-limits\n",
      "Maximum number of retries (1) exceeded.\n"
     ]
    }
   ],
   "source": [
    "# Test max retries exceeded\n",
    "@_retry_with_exponential_backoff(max_retries=1)\n",
    "def mock_func_error():\n",
    "    raise openai.error.RateLimitError\n",
    "\n",
    "\n",
    "with pytest.raises(Exception) as e:\n",
    "    mock_func_error()\n",
    "\n",
    "print(e.value)\n",
    "assert str(e.value) == \"Maximum number of retries (1) exceeded.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc8b40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class CustomAIChat:\n",
    "    \"\"\"Custom class for interacting with OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "        system_prompt: Initial system prompt to the AI model. If not passed, defaults to SYSTEM_PROMPT.\n",
    "        initial_user_prompt: Initial user prompt to the AI model.\n",
    "        params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Optional[str] = DEFAULT_MODEL,\n",
    "        user_prompt: Optional[str] = None,\n",
    "        params: Dict[str, float] = DEFAULT_PARAMS,\n",
    "    ):\n",
    "        \"\"\"Instantiates a new CustomAIChat object.\n",
    "\n",
    "        Args:\n",
    "            model: The OpenAI model to use. If not passed, defaults to gpt-3.5-turbo-16k.\n",
    "            user_prompt: The user prompt to the AI model.\n",
    "            params: Parameters to use while initiating the OpenAI chat model. DEFAULT_PARAMS used if not provided.\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.messages = [\n",
    "            {\"role\": role, \"content\": content}\n",
    "            for role, content in [\n",
    "                (\"system\", SYSTEM_PROMPT),\n",
    "                (\"user\", DEFAULT_FASTKAFKA_PROMPT),\n",
    "                (\"user\", user_prompt),\n",
    "            ]\n",
    "            if content is not None\n",
    "        ]\n",
    "        self.params = params\n",
    "\n",
    "    @_retry_with_exponential_backoff()\n",
    "    def __call__(self, user_prompt: str) -> Tuple[str, str]:\n",
    "        \"\"\"Call OpenAI API chat completion endpoint and generate a response.\n",
    "\n",
    "        Args:\n",
    "            user_prompt: A string containing user's input prompt.\n",
    "\n",
    "        Returns:\n",
    "            A tuple with AI's response message content and the total number of tokens used while generating the response.\n",
    "        \"\"\"\n",
    "        self.messages.append(\n",
    "            {\"role\": \"user\", \"content\": f\"==== APP DESCRIPTION: ====\\n\\n{user_prompt}\"}\n",
    "        )\n",
    "        logger.info(\"logger.info\")\n",
    "        logger.warning(\"logger.warning\")\n",
    "        logger.debug(\"Calling OpenAI with the below prompt message:\")\n",
    "        logger.debug(f\"\\n\\n{m}\" for m in self.messages)\n",
    "        \n",
    "        response = openai.ChatCompletion.create(\n",
    "            model=self.model,\n",
    "            messages=self.messages,\n",
    "            temperature=self.params[\"temperature\"],\n",
    "        )\n",
    "        \n",
    "        logger.debug(\"Response from OpenAI:\")\n",
    "        logger.debug(response[\"choices\"][0][\"message\"][\"content\"])\n",
    "        return (\n",
    "            response[\"choices\"][0][\"message\"][\"content\"],\n",
    "            response[\"usage\"][\"total_tokens\"],\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f5a04a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2136\n"
     ]
    }
   ],
   "source": [
    "TEST_INITIAL_USER_PROMPT = \"\"\"\n",
    "You should respond with 0, 1 or 2 and nothing else. Below are your rules:\n",
    "\n",
    "==== RULES: ====\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is not related to FastKafka or contains violence, self-harm, harassment/threatening or hate/threatening information then you should respond with 0.\n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses on what is it and its general information then you should respond with 1. \n",
    "\n",
    "If the ==== APP DESCRIPTION: ==== section is related to FastKafka but focuses how to use it and instructions to create a new app then you should respond with 2. \n",
    "\"\"\"\n",
    "\n",
    "ai = CustomAIChat(user_prompt = TEST_INITIAL_USER_PROMPT)\n",
    "response, total_tokens = ai(\"Name the tallest mountain in the world\")\n",
    "\n",
    "print(response)\n",
    "print(total_tokens)\n",
    "\n",
    "assert response == \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19a2d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@contextmanager\n",
    "def mock_openai_create(test_response):\n",
    "    mock_choices = {\n",
    "        \"choices\": [{\"message\": {\"content\": test_response}}],\n",
    "        \"usage\": {\"total_tokens\": 100},\n",
    "    }\n",
    "\n",
    "    with unittest.mock.patch(\"openai.ChatCompletion\") as mock:\n",
    "        mock.create.return_value = mock_choices\n",
    "        yield"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e27da2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a mock response\n"
     ]
    }
   ],
   "source": [
    "test_response = \"This is a mock response\"\n",
    "\n",
    "with mock_openai_create(test_response):\n",
    "    response = openai.ChatCompletion.create()\n",
    "    ret_val = response['choices'][0]['message']['content']\n",
    "    print(ret_val)\n",
    "    assert ret_val == test_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8fc36e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "class ValidateAndFixResponse:\n",
    "    \"\"\"Generates and validates response from OpenAI\n",
    "\n",
    "    Attributes:\n",
    "        generate: A callable object for generating responses.\n",
    "        validate: A callable object for validating responses.\n",
    "        max_attempts: An optional integer specifying the maximum number of attempts to generate and validate a response.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        generate: Callable[..., Any],\n",
    "        validate: Callable[..., Any],\n",
    "        max_attempts: Optional[int] = MAX_RETRIES,\n",
    "    ):\n",
    "        self.generate = generate\n",
    "        self.validate = validate\n",
    "        self.max_attempts = max_attempts\n",
    "\n",
    "    def construct_prompt_with_error_msg(\n",
    "        self,\n",
    "        prompt: str,\n",
    "        response: str,\n",
    "        errors: str,\n",
    "    ) -> str:\n",
    "        \"\"\"Construct prompt message along with the error message.\n",
    "\n",
    "        Args:\n",
    "            prompt: The original prompt string.\n",
    "            response: The invalid response string from OpenAI.\n",
    "            errors: The errors which needs to be fixed in the invalid response.\n",
    "\n",
    "        Returns:\n",
    "            A string combining the original prompt, invalid response, and the error message.\n",
    "        \"\"\"\n",
    "        prompt_with_errors = (\n",
    "            prompt\n",
    "            + f\"\\n\\n==== RESPONSE WITH ISSUES ====\\n\\n{response}\"\n",
    "            + f\"\\n\\nRead the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\\n\\n{errors}\"\n",
    "        )\n",
    "        return prompt_with_errors\n",
    "\n",
    "    def fix(self, prompt: str) -> Tuple[str, str]:\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a39d512",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some prompt\n",
      "\n",
      "==== RESPONSE WITH ISSUES ====\n",
      "\n",
      "some response\n",
      "\n",
      "Read the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\n",
      "\n",
      "error 1\n",
      "error 2\n",
      "error 3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def fixture_generate(initial_prompt):\n",
    "    return \"some response\"\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return []\n",
    "\n",
    "prompt = \"some prompt\"\n",
    "response = \"some response\"\n",
    "errors = \"\"\"error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "\n",
    "expected = \"\"\"some prompt\n",
    "\n",
    "==== RESPONSE WITH ISSUES ====\n",
    "\n",
    "some response\n",
    "\n",
    "Read the contents of ==== RESPONSE WITH ISSUES ==== section and fix the below mentioned issues:\n",
    "\n",
    "error 1\n",
    "error 2\n",
    "error 3\n",
    "\"\"\"\n",
    "\n",
    "fix_response = ValidateAndFixResponse(fixture_generate, fixture_validate)\n",
    "actual = fix_response.construct_prompt_with_error_msg(prompt, response, errors)\n",
    "print(actual)\n",
    "\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbefa15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# | export\n",
    "\n",
    "\n",
    "@patch # type: ignore\n",
    "def fix(self: ValidateAndFixResponse, prompt: str) -> Tuple[str, str]:\n",
    "    \"\"\"Fix the response from OpenAI until no errors remain or maximum number of attempts is reached.\n",
    "\n",
    "    Args:\n",
    "        prompt: The initial prompt string.\n",
    "\n",
    "    Returns:\n",
    "        str: The generated response that has passed the validation.\n",
    "\n",
    "    Raises:\n",
    "        ValueError: If the maximum number of attempts is exceeded and the response has not successfully passed the validation.\n",
    "    \"\"\"\n",
    "    iterations = 0\n",
    "    initial_prompt = prompt\n",
    "    while True:\n",
    "        response, total_tokens = self.generate(prompt)\n",
    "        errors = self.validate(response)\n",
    "        if len(errors) == 0:\n",
    "            return response, total_tokens\n",
    "        error_str = \"\\n\".join(errors)\n",
    "        prompt = self.construct_prompt_with_error_msg(\n",
    "            initial_prompt, response, error_str\n",
    "        )\n",
    "        iterations += 1\n",
    "        if self.max_attempts is not None and iterations >= self.max_attempts:\n",
    "            raise ValueError(error_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74e159a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some Valid response\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some valid prompt\"\n",
    "expected = \"Some Valid response\"\n",
    "\n",
    "def fixture_generate(initial_prompt):\n",
    "    return expected, 2\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return []\n",
    "\n",
    "v = ValidateAndFixResponse(fixture_generate, fixture_validate)\n",
    "actual, tokens = v.fix(fixture_initial_prompt)\n",
    "print(actual)\n",
    "assert actual == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbaf85e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "error 1\n",
      "error 2\n"
     ]
    }
   ],
   "source": [
    "fixture_initial_prompt = \"some invalid prompt\"\n",
    "max_attempts = 2\n",
    "\n",
    "def fixture_generate(initial_prompt):\n",
    "    return \"some invalid response\", 2\n",
    "\n",
    "def fixture_validate(response):\n",
    "    return [\"error 1\", \"error 2\"]\n",
    "\n",
    "expected = \"\"\"error 1\n",
    "error 2\"\"\"\n",
    "\n",
    "with pytest.raises(ValueError) as e:\n",
    "    v = ValidateAndFixResponse(fixture_generate, fixture_validate, max_attempts)\n",
    "    actual = v.fix(fixture_initial_prompt)\n",
    "print(e.value)\n",
    "assert str(e.value) == expected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d2d7c9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
